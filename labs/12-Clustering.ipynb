{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm works as follows:\n",
    "\n",
    "* Put each data point in its own cluster.\n",
    "* Identify the closest two clusters and combine them into one cluster.\n",
    "* Repeat the above step till all the data points are in a single cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(iris$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hclust` requires us to provide the data in the form of a distance matrix. We can do this by using `dist`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dist(iris[, 3:4], method = \"euclidean\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dist` This function computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix.\n",
    "\n",
    "In general, for a data sample of size M, the distance matrix is an M × M symmetric matrix with M × (M - 1)∕2 distinct elements. Hence for a data sample of size 150 (the number of observations in `iris`), its distance matrix has about 11,000 distinct elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters1 <- hclust(d) #defaults to complete linkage method (not covered)\n",
    "clusters1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/hclust_complete_dist.png)\n",
    "complete linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/hclust_average_dist.png)\n",
    "average or UPGMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/hclust_centroid_dist.png)\n",
    "centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(clusters1)\n",
    "rect.hclust(clusters1 , k = 3, border = 2:6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\n",
    "\n",
    "The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCut1 <- cutree(clusters1, 3)\n",
    "clusterCut1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(clusterCut1, iris$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the algorithm successfully classified all the flowers of species setosa into cluster 1, and virginica into cluster 2, but had trouble with versicolor. If you look at the original plot showing the different species, you can understand why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) + geom_point(size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Let's see if using a different clustering method will give better results. Try the `average` method on your own (add the argument `method` to the `hclust` function). Plot the respective dendogram. Compare cluster membership among, k=3 clusters, and the members between species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In k means clustering, we have to specify the number of clusters we want the data to be grouped into. The algorithm randomly assigns each observation to a cluster, and finds the centroid of each cluster. Then, the algorithm iterates through two steps:\n",
    "* Reassign data points to the cluster whose centroid is closest.\n",
    "* Calculate new centroid of each cluster.\n",
    "\n",
    "These two steps are repeated till the within cluster variation cannot be reduced any further. The within cluster variation is calculated as the sum of the euclidean distance between the data points and their respective cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(20)\n",
    "kCluster <- kmeans(iris[, 3:4], 3, nstart = 1)\n",
    "kCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(kCluster$cluster, iris$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data belonging to the setosa species got grouped into cluster 3, versicolor into cluster 1, and virginica into cluster 2. The algorithm wrongly classified two data points belonging to versicolor and four data points belonging to virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the data to see the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisCluster$cluster <- as.factor(irisCluster$cluster)\n",
    "ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kCluster$cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder the cluster assignment to be congruent with the species membership\n",
    "c1 = which(kCluster$cluster==1)\n",
    "c2 = which(kCluster$cluster==2)\n",
    "c3 = which(kCluster$cluster==3)\n",
    "\n",
    "v = vector(length=length(kCluster$cluster))\n",
    "for(i in 1:length(kCluster$cluster)){\n",
    "    if(i %in% c1 == TRUE) v[i] = 2\n",
    "    else if(i %in% c2 == TRUE) v[i] = 3\n",
    "    else v[i] = 1\n",
    "}\n",
    "v\n",
    "\n",
    "ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) +\n",
    "    geom_point(alpha = 0.4, size = 3.5) + geom_point(col = v) +\n",
    "    scale_color_manual(values = c('black', 'red', 'green'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining optimal number of clusters (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow Method for finding the optimal number of clusters\n",
    "# Compute and plot wss for k = 1 to k = 7.\n",
    "k.max <- 7\n",
    "wss <- sapply(1:k.max, \n",
    "              function(k){kmeans(iris[, 3:4], k, nstart=20)$tot.withinss})\n",
    "wss\n",
    "plot(1:k.max, wss,\n",
    "     type=\"b\", pch = 19, frame = FALSE, \n",
    "     xlab=\"Number of clusters K\",\n",
    "     ylab=\"Total within-clusters sum of squares\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#silhouette method\n",
    "silhouette_score <- function(k){\n",
    "  km <- kmeans(iris[, 3:4], centers = k, nstart=20)\n",
    "  ss <- silhouette(km$cluster, dist(iris[, 3:4]))\n",
    "  mean(ss[, 3])\n",
    "}\n",
    "k <- 2:7\n",
    "avg_sil <- sapply(k, silhouette_score)\n",
    "plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Run a K-means analysis for 2 clusters. Compare the table for membership within clusters and membership within species for this analysis to the table from the k=3 analysis. Do you notice anything interesting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thought: what biological reason might there be that versicolor and virginica are getting clustered together?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
